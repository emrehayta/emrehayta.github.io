[{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/bash/","section":"Tags","summary":"","title":"Bash","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"CLI","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/command-line/","section":"Tags","summary":"","title":"Command Line","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/debian/","section":"Tags","summary":"","title":"Debian","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/","section":"emr3.me","summary":"","title":"emr3.me","type":"page"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/filesystems/","section":"Tags","summary":"","title":"Filesystems","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":" Introduction # In the world of Linux, data storage plays a crucial role. This article will cover the fundamentals of Linux storage solutions, including filesystems, the configuration of the /etc/fstab file, and important commands for managing storage. We will also introduce Logical Volume Management (LVM), which allows for flexible handling of storage resources.\nWhat are Filesystems? # Definition: Explain what a filesystem is and its purpose (e.g., organization, storage, retrieval of data).\nTypes of Filesystems: Provide an overview of common Linux filesystems such as:\nExt4: The most widely used filesystem in Linux, known for its robustness and performance. It supports large files and has journaling capabilities to enhance data integrity. XFS: Optimized for high performance, particularly with large files and concurrent access. It is often used in enterprise environments where speed and scalability are critical. Btrfs: A modern filesystem that includes features like snapshots, built-in RAID, and self-healing capabilities. It is designed for high performance and easy management. ZFS: A powerful filesystem known for its data integrity features, compression, and snapshot capabilities (Note: A separate blog post about ZFS will be coming soon). The /etc/fstab File # Definition and Purpose: The /etc/fstab file (file system table) is a configuration file on Linux systems that defines how disk partitions, block devices, and remote filesystems should be mounted and integrated into the filesystem. It automates the mounting process at boot time, ensuring that necessary filesystems are available for use without manual intervention. Format: The format of the /etc/fstab file consists of several fields, each separated by whitespace. The typical structure includes:\nDevice: The device file (e.g., /dev/sda1) or UUID of the filesystem to be mounted. Mount Point: The directory where the filesystem will be mounted (e.g., /mnt/data). Filesystem Type: The type of filesystem (e.g., ext4, xfs, btrfs, zfs). Options: Mount options (e.g., defaults, noatime, ro for read-only). Dump: A number indicating whether the filesystem should be backed up (0 or 1). Pass: A number that indicates the order in which filesystems should be checked at boot time. Example: Here is a simple example of an /etc/fstab file:\nUUID=abcd1234-5678-90ef-ghij-klmnopqrstuv /mnt/data ext4 defaults 0 2 /dev/sdb1 /mnt/backup xfs defaults,noatime 0 0 Explanation:\nThe first line mounts an Ext4 filesystem with a specific UUID to the mount point /mnt/data with default options, and it will be checked during boot (indicated by the 2). The second line mounts an XFS filesystem from /dev/sdb1 to /mnt/backup, using the noatime option to prevent the update of access times, and it will not be checked at boot (indicated by the 0). Important Commands for Managing Storage # lsblk # Usage: Lists all block devices, showing their hierarchy and mount points. Example: Running lsblk might yield the following output:\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 500G 0 disk ├─sda1 8:1 0 200G 0 part / ├─sda2 8:2 0 300G 0 part /mnt/data Explanation: This output shows that the disk /dev/sda has two partitions: /dev/sda1 mounted as the root filesystem and /dev/sda2 mounted at /mnt/data.\nblkid # Usage: Displays the UUIDs and types of filesystems on block devices. Example: Running blkid might produce:\n/dev/sda1: UUID=\u0026#34;abcd1234-5678-90ef-ghij-klmnopqrstuv\u0026#34; TYPE=\u0026#34;ext4\u0026#34; /dev/sda2: UUID=\u0026#34;efgh5678-1234-90ab-cdef-uvwxyz123456\u0026#34; TYPE=\u0026#34;xfs\u0026#34; Explanation: This output shows the UUIDs and filesystem types for the partitions on /dev/sda.\ndf # Usage: Shows the available and used space on mounted filesystems. Example: Running df -h might yield:\nFilesystem Size Used Avail Use% Mounted on /dev/sda1 200G 10G 190G 5% / /dev/sda2 300G 50G 250G 17% /mnt/data Explanation: This output provides a human-readable format of the size, used space, available space, and usage percentage for each mounted filesystem.\nfdisk # Usage: A tool for partitioning disks. Example: Running fdisk -l might show:\nDisk /dev/sda: 500 GB, 500107862016 bytes 255 heads, 63 sectors/track, 60801 cylinders, total 976773168 sectors Explanation: This output displays information about the disk, including its size, partitioning scheme, and total sectors.\nmkfs # Usage: Used to create a filesystem on a partition. Example: To create an Ext4 filesystem on /dev/sdb1, you would run:\nmkfs.ext4 /dev/sdb1 Explanation: This command formats the specified partition (/dev/sdb1) with the Ext4 filesystem.\nLogical Volume Management (LVM) # What is LVM? # Logical Volume Management (LVM) is a system for managing disk space that allows for flexible disk partitioning and management. It provides the ability to create, resize, and delete logical volumes without worrying about the underlying physical storage.\nCore Concepts # LVM consists of three main components:\nPhysical Volumes (PV): The actual disk drives or partitions. Volume Groups (VG): A collection of physical volumes that can be allocated as logical volumes. Logical Volumes (LV): Virtual partitions created from the space allocated in volume groups. Here are examples of important LVM commands: # Creating a Physical Volume # To initialize a physical volume on /dev/sdb, you would run:\npvcreate /dev/sdb Creating a Volume Group # To create a volume group named vg_data using the physical volume /dev/sdb, run:\nvgcreate vg_data /dev/sdb Creating a Logical Volume # To create a logical volume named lv_backup of size 50G within vg_data, run:\nlvcreate -n lv_backup -L 50G vg_data Listing Logical/Physical Volumes and Volume Groups: # To display information about all logical volumes, use:\nlvs #To display information about all logical volumes pvs #To view all physical volumes and their attributes vgs #To display all volume groups and their properties Conclusion # Understanding Linux storage is crucial for managing an effective and efficient system. From filesystems and the /etc/fstab configuration to essential commands and Logical Volume Management, these concepts equip you with the knowledge needed to handle data storage confidently.\nResources # Include links to further resources, tutorials, and official documentation to give readers the opportunity to deepen their knowledge. Feel free to use this version for your blog post. If you have any more requests or need further modifications, just let me know!\n","date":"18 October 2024","externalUrl":null,"permalink":"/posts/linux-filesystems/","section":"Posts","summary":"","title":"Linux Storage: An Introduction to Filesystems, fstab, and Essential Commands","type":"posts"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/lvm/","section":"Tags","summary":"","title":"LVM","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/terminal/","section":"Tags","summary":"","title":"Terminal","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/ubuntu/","section":"Tags","summary":"","title":"Ubuntu","type":"tags"},{"content":"","date":"18 October 2024","externalUrl":null,"permalink":"/tags/unix/","section":"Tags","summary":"","title":"Unix","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/containerization/","section":"Tags","summary":"","title":"Containerization","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/custom-docker-image/","section":"Tags","summary":"","title":"Custom Docker Image","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":" Introduction to Docker # Docker has become an essential tool in modern software development. Combined with Docker Compose, managing multi-container applications is straightforward. In this blog post, I will show you how to work with Docker and Docker Compose, including how to run a simple Nginx web server and build your own Docker images.\nWhat is Docker? # Docker is a containerization platform that allows developers to run applications in isolated environments called containers. These containers package all the dependencies required to run an application, ensuring consistency across different environments.\nWhat is Docker Compose? # Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to configure your application\u0026rsquo;s services, networks, and volumes in a single YAML file, making it easier to manage complex applications.\nExample Project Structure # Let’s create a simple project using Docker and Docker Compose to run an Nginx web server.\n1. Project Directory Structure: # my-nginx-app/ ├── docker-compose.yml ├── deploy.sh └── index.html 2. Sample index.html # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Welcome to Nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, Docker with Nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;This is a simple web page served by Nginx.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 3. Sample docker-compose.yml # version: \u0026#39;3\u0026#39; services: web: image: nginx:latest ports: - \u0026#34;8080:80\u0026#34; volumes: - ./index.html:/usr/share/nginx/html/index.html Deploy Script: deploy.sh # Now, let’s create a simple shell script to start the Nginx server using Docker Compose.\n#!/bin/bash # Start the Nginx application using Docker Compose docker-compose up Running the Application # Make sure you have Docker and Docker Compose installed on your machine. Navigate to the project directory: cd my-nginx-app Make the deploy.sh script executable: chmod +x deploy.sh Run the deploy script: ./deploy.sh Your Nginx server will start, and you can access it by visiting http://localhost:8080 in your web browser. You should see the message \u0026ldquo;Hello, Docker with Nginx!\u0026rdquo; displayed on the page.\nBuilding Custom Docker Images # If you want to build your own Docker image, you can create a simple Dockerfile. Here’s an example of how to create a custom Nginx image that serves your own HTML content.\nSample Dockerfile # Use the official Nginx image as a base FROM nginx:latest # Copy custom HTML file to the Nginx HTML directory COPY index.html /usr/share/nginx/html/index.html Update docker-compose.yml version: \u0026#39;3\u0026#39; services: web: build: . ports: - \u0026#34;8080:80\u0026#34; Modify the deploy.sh script: Update the script to build the image before starting the containers: #!/bin/bash # Build and start the Nginx application using Docker Compose docker-compose up --build Conclusion # Docker and Docker Compose simplify the development and deployment process, allowing you to manage applications efficiently. In this article, we covered the basics of Docker and Docker Compose, how to run a simple Nginx web server, and how to create a custom Docker image.\nFeel free to expand this example further by adding more services or customizing the content as needed!\n","date":"11 October 2024","externalUrl":null,"permalink":"/posts/docker-basics/","section":"Posts","summary":"","title":"Docker + Docker Compose: Getting Started and Best Practices","type":"posts"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/docker-compose/","section":"Tags","summary":"","title":"Docker Compose","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/docker-tutorial/","section":"Tags","summary":"","title":"Docker Tutorial","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/nginx/","section":"Tags","summary":"","title":"Nginx","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/shell-script/","section":"Tags","summary":"","title":"Shell Script","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/software-deployment/","section":"Tags","summary":"","title":"Software Deployment","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/web-development/","section":"Tags","summary":"","title":"Web Development","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":" Introduction to Ansible # Ansible is an open-source automation tool that simplifies the process of IT orchestration, configuration management, and application deployment. It helps automate repetitive tasks and manage complex IT environments, making it an essential tool in the DevOps toolkit.\nWhy Ansible? # Ansible\u0026rsquo;s popularity stems from its ease of use and powerful features:\nAgentless Architecture: Unlike other automation tools, Ansible doesn\u0026rsquo;t require any agent software to be installed on target systems. It only requires SSH access, which makes setup and management easier. Simple YAML Syntax: Ansible uses a simple, human-readable language called YAML, making it accessible for anyone to learn and write automation scripts without deep programming skills. Key Concepts of Ansible # Playbooks # Playbooks are Ansible\u0026rsquo;s configuration, deployment, and orchestration language. They are written in YAML and describe the steps to achieve a particular outcome.\nInventories # The inventory file lists the systems you want to manage. It can be static or dynamic, and it allows Ansible to know which hosts it needs to communicate with.\nModules # Ansible comes with hundreds of built-in modules for different tasks, such as managing files, installing software packages, or interacting with cloud providers.\nRoles # Roles provide a way to organize playbooks and related tasks, variables, files, and handlers in a reusable and structured format. They make it easier to reuse code and organize your automation logic.\nProject Structure Overview # Before diving into Ansible, it\u0026rsquo;s important to understand how to organize your Ansible project. Here is a typical project structure that you might use:\n├── inventory ├── roles │ ├── common │ │ ├── tasks │ │ │ └── main.yml ├── playbook.yml Getting Started with Ansible # Installation # Make sure you have Python and pip installed, then run:\npip install ansible After the installation is complete, you can verify it by checking the version:\nansible --version A Simple Playbook Example # This playbook defines a task that installs nginx on the web host group. The become: yes directive means that the task should run with elevated privileges.\n--- - name: Install Nginx on web server hosts: web become: yes tasks: - name: Install Nginx apt: name: nginx state: present Practical Examples # Install a Web Server # --- - name: Setup Apache Web Server hosts: web become: yes tasks: - name: Update apt repository apt: update_cache: yes - name: Install Apache2 apt: name: apache2 state: present Deploying Software on Multiple Hosts # To deploy a piece of software across multiple servers, you can define them in an inventory file and create a playbook similar to the one above. Ansible will connect to each host and execute the tasks, ensuring consistency.\nExample Inventory File # The inventory file defines the hosts that Ansible will manage. You can list your servers and organize them into groups.\nInventory File (inventory.ini) # [web] webserver1 ansible_host=192.168.1.10 ansible_user=ubuntu webserver2 ansible_host=192.168.1.11 ansible_user=ubuntu [database] dbserver1 ansible_host=192.168.1.20 ansible_user=ubuntu [all:vars] ansible_python_interpreter=/usr/bin/python3 [web]: Defines a group called web containing two servers (webserver1 and webserver2). [database]: Defines a group called database containing one server (dbserver1). [all]: This section defines variables that apply to all hosts, such as the Python interpreter to use. Example Role # Roles allow you to organize your tasks, files, handlers, and variables in a structured way, making it easy to reuse and maintain automation.\nRole Directory Structure # Here is an example of a role to install and configure nginx:\nroles/ └── nginx/ ├── tasks/ │ └── main.yml ├── handlers/ │ └── main.yml ├── templates/ │ └── nginx.conf.j2 ├── files/ ├── vars/ │ └── main.yml └── defaults/ └── main.yml Role Components # Tasks (tasks/main.yml) - The tasks file contains the list of actions to perform: --- - name: Install Nginx apt: name: nginx state: present notify: Restart Nginx - name: Copy Nginx configuration file template: src: nginx.conf.j2 dest: /etc/nginx/nginx.conf mode: \u0026#39;0644\u0026#39; The first task installs nginx and triggers a handler to restart it. The second task copies the nginx configuration file from a template. Handlers (handlers/main.yml) - Handlers are used to trigger actions when notified by tasks: --- - name: Restart Nginx service: name: nginx state: restarted Templates (templates/nginx.conf.j2) - Templates allow you to create dynamic configuration files: user www-data; worker_processes auto; pid /run/nginx.pid; events { worker_connections 768; } http { include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; } Variables (vars/main.yml) - Define variables specific to this role: --- nginx_user: www-data worker_processes: auto Defaults (defaults/main.yml) - Default variables that can be overridden: --- worker_connections: 768 keepalive_timeout: 65 Using the Role in a Playbook # You can use the role in your playbook like this:\nPlaybook (site.yml)\nThis playbook applies the nginx role to all hosts in the web group. --- - name: Configure web servers hosts: web become: yes roles: - nginx Tips and Best Practices # Project Structure: Organize your playbooks in a structured format. Organize your playbooks in a structured format. Refer to the project structure overview for guidance on how to set up your Ansible project effectively.\nReusability: Use roles to ensure your playbooks are modular and reusable. This helps reduce redundancy and makes your automation scripts easier to maintain.\nSensitive Data: Use Ansible Vault to encrypt sensitive information like passwords and API keys. This keeps your configuration secure.\nSummary and Further Resources # Ansible is a powerful tool for IT automation that makes infrastructure management more efficient. With its agentless architecture and simple YAML syntax, it’s ideal for both beginners and experienced engineers. To dive deeper, check out the official Ansible Documentation and start experimenting with more complex playbooks and roles.\n","date":"4 October 2024","externalUrl":null,"permalink":"/posts/ansible-basics/","section":"Posts","summary":"","title":"Ansible Basics: Getting Started with Automation","type":"posts"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/configuration-management/","section":"Tags","summary":"","title":"Configuration Management","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/it-infrastructure/","section":"Tags","summary":"","title":"IT Infrastructure","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/open-source-tools/","section":"Tags","summary":"","title":"Open Source Tools","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/playbooks/","section":"Tags","summary":"","title":"Playbooks","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/yaml/","section":"Tags","summary":"","title":"YAML","type":"tags"},{"content":" Introduction # Linux offers a multitude of powerful commands that make working on the command line efficient and effective. Whether you are managing files, monitoring system resources, or networking, knowing the right commands can save you a lot of time. This article introduces essential Linux commands and will be regularly updated with new commands and useful tips.\nFile and Directory Management # ls # ls -lah List files and folder ls file* List only entries beginning with file ls *.txt List only entries ending with .txt -l Long format: permissions, owner, group, size -a Include all files: hidden files -h Human-readable: KB, MB, or GB format cd # cd /path/to/folder Change to the specified directory cd .. Go one directory up (parent directory) cd - Switch to the previous directory cd ~ Switch to the home directory cp # cp file destination Copy a file to the destination cp -r folder destination Recursively copy a folder cp -i source destination Prompt before overwriting cp -u source destination Copy only if source is newer than destination mv # mv source destination Move or rename a file/folder mv -i source destination Prompt before overwriting mv -u source destination Move only if source is newer than destination File and Text Manipulation # cat # cat file Display the contents of a file cat file1 file2 Concatenate and display multiple files cat \u0026gt; file Create a new file and write to it cat \u0026gt;\u0026gt; file Append to an existing file grep # grep 'pattern' file Search for a pattern in a file grep -i 'pattern' file Case-insensitive search grep -r 'pattern' folder Recursively search in a folder grep -v 'pattern' file Display lines not matching the pattern head # head file Display the first 10 lines of a file head -n 20 file Display the first 20 lines of a file head -c 50 file Display the first 50 bytes of a file head -v file Always show file name before output tail # tail file Display the last 10 lines of a file tail -n 20 file Display the last 20 lines of a file tail -f file Follow and display appended data in real-time tail -c 50 file Display the last 50 bytes of a file Advanced Text Manipulation # awk # awk '{print $1}' file Print the first column of a file awk -F, '{print $2}' file Use a comma as the field separator and print the second column awk '/pattern/' file Print lines that match a specific pattern awk 'NR==1 {print $0}' file Print only the first line of the file sed # sed 's/old/new/g' file Replace all occurrences of \u0026ldquo;old\u0026rdquo; with \u0026ldquo;new\u0026rdquo; in a file sed -n '1,5p' file Print only lines 1 to 5 of a file sed '/pattern/d' file Delete lines that match a specific pattern sed -i 's/old/new/g' file In-place replace \u0026ldquo;old\u0026rdquo; with \u0026ldquo;new\u0026rdquo; in the file System Information and Process Management # top # top Display real-time system resource usage top -u user Show processes for a specific user top -p PID Monitor a specific process by PID top -n 1 Display output once and exit htop # htop Interactive process viewer with more details htop -u user Show processes for a specific user htop -p PID Monitor a specific process by PID htop --tree Display processes in a tree view df # df Display disk space usage for all mounted filesystems df -h Show disk space usage in human-readable format (e.g., MB, GB) df -T Display filesystem type df -i Show inode usage instead of block usage Networking # ping # ping hostname_or_ip Send ICMP echo requests to test network connectivity ping -c 4 hostname_or_ip Send 4 ICMP echo requests and stop ping -i 2 hostname_or_ip Set the interval between sending packets to 2 seconds ping -t hostname_or_ip Ping continuously (until stopped manually) ifconfig # Install: sudo apt install net-tools ifconfig Display network interfaces and their configurations ifconfig eth0 Show details for a specific interface (e.g., eth0) ifconfig eth0 up Enable the specified interface ifconfig eth0 down Disable the specified interface curl # Install: sudo apt install curl curl url Fetch the content of the specified URL curl -O url Download a file from the URL curl -I url Fetch only the headers of the specified URL curl -d \u0026quot;data\u0026quot; url Send POST data to the specified URL netstat # netstat Display network connections, routing tables, interface statistics netstat -tuln Show listening TCP and UDP ports netstat -i Display network interface statistics netstat -r Display the kernel routing table nc (netcat) # nc -l 12345 Listen on port 12345 for incoming connections nc -v hostname_or_ip 12345 Connect to a specified host and port nc -zv hostname_or_ip 80 Check if a specific port is open (e.g., port 80) nc -w 5 hostname_or_ip 12345 Set a timeout of 5 seconds for the connection telnet # Install: sudo apt install telnet telnet hostname_or_ip port Open a connection to a specific host and port telnet localhost 25 Test connection to a local mail server (port 25) telnet hostname_or_ip Open a connection to a specific host (default port 23) telnet ? Display available telnet commands dig # Install: sudo apt install dnsutils dig domain Perform a DNS lookup for the specified domain dig +short domain Display a simplified output (just the IP) dig @dns_server domain Perform a DNS lookup using a specific DNS server dig -x ip_address Perform a reverse DNS lookup for the IP address nslookup # nslookup domain Query DNS information for the specified domain nslookup Enter interactive mode to perform multiple queries nslookup domain dns_server Query the domain using a specific DNS server nslookup -type=any domain Query for all available DNS record types Updates # September 27, 2024: Added section on networking commands (ping, curl). October 15, 2024: Added new examples for grep and awk. October 18, 2024: Updated structure of blog article ","date":"28 September 2024","externalUrl":null,"permalink":"/posts/linux-commands/","section":"Posts","summary":"","title":"Essential Linux Commands: A Growing Cheat Sheet for Everyday Use","type":"posts"},{"content":" Privacy Policy # This privacy policy explains how I handle visitors\u0026rsquo; data and which tools are used on this website, such as Google Analytics. Protecting your personal data is very important to me, and I strive to ensure compliance with data protection regulations.\n1. General Information # The purpose of this privacy policy is to inform you about the type, scope, and purpose of the collection and use of personal data on this website. I, Emre Hayta, am the owner of this website and responsible for data protection.\n2. Data Collection # a. Server Log Files # When visiting this website, the web server automatically collects and stores information in server log files. These include:\nBrowser type and version Operating system used Referrer URL Hostname of the accessing computer Time of the server request IP address (anonymized) This data cannot be attributed to specific individuals and is only used for statistical purposes, to improve the website, and for security reasons.\nb. Google Analytics # I use Google Analytics to analyze user behavior on my website. Google Analytics uses cookies that enable an analysis of how you use the website. The information generated by the cookies about your use of this website is usually transmitted to a Google server in the USA and stored there. However, IP anonymization is activated on this website, meaning that your IP address will be truncated beforehand within the European Union or other parties to the Agreement on the European Economic Area.\nc. Contact Form and Email Communication # If you contact me via a contact form or by email, the information you provide (such as your email address, name, and message content) will be stored for the purpose of processing your request and in case of follow-up questions. This data will not be shared without your permission.\n3. Cookies # This website uses cookies to make the site more user-friendly, effective, and secure. Cookies are small text files that are stored on your device. Some of these cookies are \u0026ldquo;session cookies,\u0026rdquo; which are deleted after your visit, while others remain on your device until deleted. You can configure your browser to inform you about the use of cookies and allow them only in individual cases.\n4. Your Rights # You have the right to:\nAccess information about your personal data stored by me. Rectify incorrect data. Delete your personal data, provided there is no legal obligation to retain it. Restrict processing of your data. Object to data processing under certain circumstances. To exercise any of these rights, please contact me using the details provided in the imprint.\n5. Data Security # I take appropriate technical and organizational measures to ensure the security of your data and protect it against unauthorized access, alteration, or destruction.\n6. Changes to This Privacy Policy # I may update this privacy policy from time to time to reflect changes in legal requirements or my services. The current version will always be available on this website.\n7. Contact Information # If you have any questions about this privacy policy or the processing of your personal data, feel free to contact me via the contact form on this website.\nBy providing all this essential information, the privacy policy now complies with general legal standards, covering the types of data collected, the use of cookies, rights of the users, and other basic information relevant for ensuring transparency and compliance with data protection regulations.\n","date":"28 September 2024","externalUrl":null,"permalink":"/privacy/","section":"emr3.me","summary":"","title":"Privacy Policy","type":"page"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS","type":"tags"},{"content":" Introduction # Amazon Web Services (AWS) is one of the leading cloud platforms, offering a wide range of services that allow businesses and developers to build, manage, and scale applications. For beginners, AWS can seem overwhelming at first, with over 200 different services to choose from. In this post, I’ll give you an overview of some commonly used AWS services and share useful tips for getting started.\nCommonly Used AWS Services # 1. Amazon EC2 (Elastic Compute Cloud) # What is it? EC2 provides scalable virtual servers in the cloud. You can choose from various instance types optimized for different workloads. When to use it? If you want to host your own applications or websites, EC2 gives you the flexibility to create and manage servers. 2. Amazon S3 (Simple Storage Service) # What is it? S3 is an object storage service ideal for storing large amounts of data like backups, images, or videos. When to use it? If you need secure, scalable storage. S3 is often used for backups and hosting static websites. 3. Amazon RDS (Relational Database Service) # What is it? RDS is a managed database service supporting multiple relational database engines like MySQL, PostgreSQL, and MariaDB. When to use it? If you need a relational database for your application but don’t want the hassle of managing it yourself. 4. Amazon Lambda # What is it? Lambda allows you to run code without managing servers. You only pay for the time your code is actually running. When to use it? When you want to build serverless applications or execute small, event-driven tasks. 5. Amazon CloudFront # What is it? CloudFront is a Content Delivery Network (CDN) that speeds up the delivery of content by caching copies at multiple locations globally. When to use it? If you have a global user base and want to optimize your website or app’s load times. 6. AWS IAM (Identity and Access Management) # What is it? IAM allows you to manage users, groups, and their permissions within AWS. When to use it? To ensure the security and access control of your AWS resources. Tips for Beginners # 1. Take Advantage of the AWS Free Tier # AWS offers a free tier for many services, allowing you to explore AWS without incurring extra costs. Be mindful that the free tier is limited in terms of time and usage.\n2. Learn the Basics of IAM # Security should be your top priority. Make sure you understand how IAM works to keep your AWS resources safe from unauthorized access.\n3. Utilize the Documentation and Tutorials # AWS provides comprehensive documentation and step-by-step tutorials. These resources are especially helpful for beginners looking to get familiar with specific services.\n4. Automate with AWS CLI and CloudFormation # For repeatable tasks, you can use the AWS Command Line Interface (CLI) or Infrastructure as Code (IaC) tools like AWS CloudFormation to automate your infrastructure.\n5. Set Up Budget Alarms # AWS offers flexible pricing models, but costs can quickly add up. Set budget alarms to keep track of your spending and avoid surprises.\nConclusion # Getting started with AWS can be challenging, but with the right resources and a clear focus, you can quickly gain momentum. The AWS services introduced here are some of the most commonly used by beginners. By familiarizing yourself with these and following the tips outlined, you\u0026rsquo;ll be well on your way to working successfully with AWS.\nHave any questions or want to learn more about specific services? Feel free to contact me!\n","date":"25 September 2024","externalUrl":null,"permalink":"/posts/aws-beginner-guide/","section":"Posts","summary":"","title":"AWS for Beginners: A Comprehensive Guide to Common Services and Essential Tips","type":"posts"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/cloudfront/","section":"Tags","summary":"","title":"Cloudfront","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/ec2/","section":"Tags","summary":"","title":"EC2","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/lambda/","section":"Tags","summary":"","title":"Lambda","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/rds/","section":"Tags","summary":"","title":"RDS","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/s3/","section":"Tags","summary":"","title":"S3","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/blogging/","section":"Tags","summary":"","title":"Blogging","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/github-pages/","section":"Tags","summary":"","title":"GitHub Pages","type":"tags"},{"content":" Introduction # In today\u0026rsquo;s fast-paced digital world, having your own blog is a great way to share knowledge, experiences, and ideas. In this guide, I will show you how to set up a static website using Hugo and host it for free on GitHub Pages. Hugo is a fast and flexible static site generator, and GitHub Pages makes it easy to host the site directly from your GitHub repository.\nBy the end of this guide, you will have a fully functional blog running on Hugo and deployed through GitHub Pages.\nWhat is Hugo? # Hugo is a powerful and fast Static Site Generator (SSG). It allows you to create websites that are completely static, which makes them fast, secure, and easy to host. With Hugo, you can write content in Markdown and easily customize your site\u0026rsquo;s design using templates and themes.\nWhy GitHub Pages? # GitHub Pages is a hosting service from GitHub that lets you publish websites directly from a GitHub repository. It\u0026rsquo;s particularly useful for projects, personal blogs, and documentation. The integration with GitHub makes it easy to manage changes and keep your site up to date.\nPrerequisites # Before we get started, make sure you have the following installed on your system:\nGit Hugo GitHub Account Step 1: Setting Up Your Hugo Site # First, create a new Hugo site on your local machine. Navigate to the directory where you want to store your blog files and run the following commands:\nhugo new site my-blog cd my-blog This will create the structure of your Hugo site in a directory called my-blog. Next, you\u0026rsquo;ll need to add a theme. In this case, let\u0026rsquo;s use the popular \u0026ldquo;Blowfish\u0026rdquo; theme. Run:\ngit init git submodule add https://github.com/nunocoracao/blowfish.git themes/blowfish echo \u0026#39;theme = \u0026#34;blowfish\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml Step 2: Create Your First Blog Post # Now that the site structure is ready, create your first blog post by running:\nhugo new posts/my-first-post.md This will generate a new Markdown file for your post inside the content/posts/ directory. You can edit this file with your favorite text editor and add the content for your first post.\nStep 3: Preview Your Site Locally # Before deploying your blog, you can preview it locally to see how it looks. In the root directory of your Hugo project, run:\nhugo server -D Open a browser and go to http://localhost:1313/ to see your site in action.\nStep 4: Preparing for Deployment to GitHub Pages # Now it’s time to deploy your blog to GitHub Pages.\n4.1 Create a GitHub Repository # Go to GitHub and create a new repository. You can name it .github.io to use it for GitHub Pages. If you want to host it on a subdomain, you can use another name like my-blog.\n4.2 Build the Static Site manually # Run the following command to build your site:\nhugo --minify --themesDir ../.. -d docs --baseURL https://username.github.io/ This will create a docs/ directory with all the static files of your site.\n4.3 Push to Github # cd my-blog git init git remote add origin https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repository\u0026gt;.git git add . git commit -m \u0026#34;Initial commit\u0026#34; git push -u origin main Step 5: Automating the Build with GitHub Actions # Instead of manually building and deploying your site every time you make a change, you can automate this process with GitHub Actions.\n5.1 Create the GitHub Action Workflow # In the root directory of your project (not the docs folder), create a directory called .github/workflows (if it does not exist already):\nmkdir -p .github/workflows Create a new file inside the workflows directory called gh-pages.yml and add the following content (adjust the URL):\ngh-pages.yml # Sample workflow for building and deploying a Hugo site to GitHub Pages name: Blowfish Docs Deploy on: # Runs on pushes targeting the default branch push: branches: [\u0026#34;main\u0026#34;] # # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read pages: write id-token: write # Allow one concurrent deployment concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: true # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.102.3 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_Linux-64bit.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Checkout uses: actions/checkout@v4 with: submodules: recursive - name: Setup Pages id: pages uses: actions/configure-pages@v5 - name: Build with Hugo env: HUGO_ENVIRONMENT: production HUGO_ENV: production run: | hugo --minify --themesDir ../.. -d docs --baseURL https://username.github.io/ echo \u0026#34;Build completed\u0026#34; ls -la docs - name: Upload artifact uses: actions/upload-pages-artifact@v3 with: path: ./docs # Deployment job deploy: environment: name: github-pages url: https://emrehayta.github.io/ runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v4 5.2 Commit the Workflow # Commit and push the workflow file to your repository:\ngit add .github/workflows/gh-pages.yml git commit -m \u0026#34;Add GitHub Actions workflow for Hugo\u0026#34; git push origin main Step 6: Configure GitHub Pages # In your GitHub repository, go to Settings Scroll down to the GitHub Pages section. Under Source, select the docs/ folder in our case (depending on your setup) Save the settings, and your site will be live! Conclusion # Congratulations! You\u0026rsquo;ve successfully set up a blog using Hugo and GitHub Pages. From here, you can continue customizing your theme, adding posts, and growing your site. Hugo and GitHub Pages offer a great, scalable way to maintain a static blog with minimal costs and maximum flexibility.\nHappy blogging!\n","date":"20 September 2024","externalUrl":null,"permalink":"/posts/hugo-article/","section":"Posts","summary":"","title":"Guide to Setting Up a Blog with Hugo and GitHub Pages","type":"posts"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]