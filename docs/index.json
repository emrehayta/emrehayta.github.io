[{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/containerization/","section":"Tags","summary":"","title":"Containerization","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/custom-docker-image/","section":"Tags","summary":"","title":"Custom Docker Image","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":" Introduction to Docker # Docker has become an essential tool in modern software development. Combined with Docker Compose, managing multi-container applications is straightforward. In this blog post, I will show you how to work with Docker and Docker Compose, including how to run a simple Nginx web server and build your own Docker images.\nWhat is Docker? # Docker is a containerization platform that allows developers to run applications in isolated environments called containers. These containers package all the dependencies required to run an application, ensuring consistency across different environments.\nWhat is Docker Compose? # Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to configure your application\u0026rsquo;s services, networks, and volumes in a single YAML file, making it easier to manage complex applications.\nExample Project Structure # Let’s create a simple project using Docker and Docker Compose to run an Nginx web server.\n1. Project Directory Structure: # my-nginx-app/ ├── docker-compose.yml ├── deploy.sh └── index.html 2. Sample index.html # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Welcome to Nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, Docker with Nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;This is a simple web page served by Nginx.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 3. Sample docker-compose.yml # version: \u0026#39;3\u0026#39; services: web: image: nginx:latest ports: - \u0026#34;8080:80\u0026#34; volumes: - ./index.html:/usr/share/nginx/html/index.html Deploy Script: deploy.sh # Now, let’s create a simple shell script to start the Nginx server using Docker Compose.\n#!/bin/bash # Start the Nginx application using Docker Compose docker-compose up Running the Application # Make sure you have Docker and Docker Compose installed on your machine. Navigate to the project directory: cd my-nginx-app Make the deploy.sh script executable: chmod +x deploy.sh Run the deploy script: ./deploy.sh Your Nginx server will start, and you can access it by visiting http://localhost:8080 in your web browser. You should see the message \u0026ldquo;Hello, Docker with Nginx!\u0026rdquo; displayed on the page.\nBuilding Custom Docker Images # If you want to build your own Docker image, you can create a simple Dockerfile. Here’s an example of how to create a custom Nginx image that serves your own HTML content.\nSample Dockerfile # Use the official Nginx image as a base FROM nginx:latest # Copy custom HTML file to the Nginx HTML directory COPY index.html /usr/share/nginx/html/index.html Update docker-compose.yml version: \u0026#39;3\u0026#39; services: web: build: . ports: - \u0026#34;8080:80\u0026#34; Modify the deploy.sh script: Update the script to build the image before starting the containers: #!/bin/bash # Build and start the Nginx application using Docker Compose docker-compose up --build Conclusion # Docker and Docker Compose simplify the development and deployment process, allowing you to manage applications efficiently. In this article, we covered the basics of Docker and Docker Compose, how to run a simple Nginx web server, and how to create a custom Docker image.\nFeel free to expand this example further by adding more services or customizing the content as needed!\n","date":"11 October 2024","externalUrl":null,"permalink":"/posts/docker-basics/","section":"Posts","summary":"","title":"Docker + Docker Compose: Getting Started and Best Practices","type":"posts"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/docker-compose/","section":"Tags","summary":"","title":"Docker Compose","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/docker-tutorial/","section":"Tags","summary":"","title":"Docker Tutorial","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/","section":"emr3.me","summary":"","title":"emr3.me","type":"page"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/nginx/","section":"Tags","summary":"","title":"Nginx","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/shell-script/","section":"Tags","summary":"","title":"Shell Script","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/software-deployment/","section":"Tags","summary":"","title":"Software Deployment","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"11 October 2024","externalUrl":null,"permalink":"/tags/web-development/","section":"Tags","summary":"","title":"Web Development","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":" Introduction to Ansible # Ansible is an open-source automation tool that simplifies the process of IT orchestration, configuration management, and application deployment. It helps automate repetitive tasks and manage complex IT environments, making it an essential tool in the DevOps toolkit.\nWhy Ansible? # Ansible\u0026rsquo;s popularity stems from its ease of use and powerful features:\nAgentless Architecture: Unlike other automation tools, Ansible doesn\u0026rsquo;t require any agent software to be installed on target systems. It only requires SSH access, which makes setup and management easier. Simple YAML Syntax: Ansible uses a simple, human-readable language called YAML, making it accessible for anyone to learn and write automation scripts without deep programming skills. Key Concepts of Ansible # Playbooks # Playbooks are Ansible\u0026rsquo;s configuration, deployment, and orchestration language. They are written in YAML and describe the steps to achieve a particular outcome.\nInventories # The inventory file lists the systems you want to manage. It can be static or dynamic, and it allows Ansible to know which hosts it needs to communicate with.\nModules # Ansible comes with hundreds of built-in modules for different tasks, such as managing files, installing software packages, or interacting with cloud providers.\nRoles # Roles provide a way to organize playbooks and related tasks, variables, files, and handlers in a reusable and structured format. They make it easier to reuse code and organize your automation logic.\nProject Structure Overview # Before diving into Ansible, it\u0026rsquo;s important to understand how to organize your Ansible project. Here is a typical project structure that you might use:\n├── inventory ├── roles │ ├── common │ │ ├── tasks │ │ │ └── main.yml ├── playbook.yml Getting Started with Ansible # Installation # Make sure you have Python and pip installed, then run:\npip install ansible After the installation is complete, you can verify it by checking the version:\nansible --version A Simple Playbook Example # This playbook defines a task that installs nginx on the web host group. The become: yes directive means that the task should run with elevated privileges.\n--- - name: Install Nginx on web server hosts: web become: yes tasks: - name: Install Nginx apt: name: nginx state: present Practical Examples # Install a Web Server # --- - name: Setup Apache Web Server hosts: web become: yes tasks: - name: Update apt repository apt: update_cache: yes - name: Install Apache2 apt: name: apache2 state: present Deploying Software on Multiple Hosts # To deploy a piece of software across multiple servers, you can define them in an inventory file and create a playbook similar to the one above. Ansible will connect to each host and execute the tasks, ensuring consistency.\nExample Inventory File # The inventory file defines the hosts that Ansible will manage. You can list your servers and organize them into groups.\nInventory File (inventory.ini) # [web] webserver1 ansible_host=192.168.1.10 ansible_user=ubuntu webserver2 ansible_host=192.168.1.11 ansible_user=ubuntu [database] dbserver1 ansible_host=192.168.1.20 ansible_user=ubuntu [all:vars] ansible_python_interpreter=/usr/bin/python3 [web]: Defines a group called web containing two servers (webserver1 and webserver2). [database]: Defines a group called database containing one server (dbserver1). [all]: This section defines variables that apply to all hosts, such as the Python interpreter to use. Example Role # Roles allow you to organize your tasks, files, handlers, and variables in a structured way, making it easy to reuse and maintain automation.\nRole Directory Structure # Here is an example of a role to install and configure nginx:\nroles/ └── nginx/ ├── tasks/ │ └── main.yml ├── handlers/ │ └── main.yml ├── templates/ │ └── nginx.conf.j2 ├── files/ ├── vars/ │ └── main.yml └── defaults/ └── main.yml Role Components # Tasks (tasks/main.yml) - The tasks file contains the list of actions to perform: --- - name: Install Nginx apt: name: nginx state: present notify: Restart Nginx - name: Copy Nginx configuration file template: src: nginx.conf.j2 dest: /etc/nginx/nginx.conf mode: \u0026#39;0644\u0026#39; The first task installs nginx and triggers a handler to restart it. The second task copies the nginx configuration file from a template. Handlers (handlers/main.yml) - Handlers are used to trigger actions when notified by tasks: --- - name: Restart Nginx service: name: nginx state: restarted Templates (templates/nginx.conf.j2) - Templates allow you to create dynamic configuration files: user www-data; worker_processes auto; pid /run/nginx.pid; events { worker_connections 768; } http { include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; } Variables (vars/main.yml) - Define variables specific to this role: --- nginx_user: www-data worker_processes: auto Defaults (defaults/main.yml) - Default variables that can be overridden: --- worker_connections: 768 keepalive_timeout: 65 Using the Role in a Playbook # You can use the role in your playbook like this:\nPlaybook (site.yml)\nThis playbook applies the nginx role to all hosts in the web group. --- - name: Configure web servers hosts: web become: yes roles: - nginx Tips and Best Practices # Project Structure: Organize your playbooks in a structured format. Organize your playbooks in a structured format. Refer to the project structure overview for guidance on how to set up your Ansible project effectively.\nReusability: Use roles to ensure your playbooks are modular and reusable. This helps reduce redundancy and makes your automation scripts easier to maintain.\nSensitive Data: Use Ansible Vault to encrypt sensitive information like passwords and API keys. This keeps your configuration secure.\nSummary and Further Resources # Ansible is a powerful tool for IT automation that makes infrastructure management more efficient. With its agentless architecture and simple YAML syntax, it’s ideal for both beginners and experienced engineers. To dive deeper, check out the official Ansible Documentation and start experimenting with more complex playbooks and roles.\n","date":"4 October 2024","externalUrl":null,"permalink":"/posts/ansible-basics/","section":"Posts","summary":"","title":"Ansible Basics: Getting Started with Automation","type":"posts"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/configuration-management/","section":"Tags","summary":"","title":"Configuration Management","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/it-infrastructure/","section":"Tags","summary":"","title":"IT Infrastructure","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/open-source-tools/","section":"Tags","summary":"","title":"Open Source Tools","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/playbooks/","section":"Tags","summary":"","title":"Playbooks","type":"tags"},{"content":"","date":"4 October 2024","externalUrl":null,"permalink":"/tags/yaml/","section":"Tags","summary":"","title":"YAML","type":"tags"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/bash/","section":"Tags","summary":"","title":"Bash","type":"tags"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"CLI","type":"tags"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/command-line/","section":"Tags","summary":"","title":"Command Line","type":"tags"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/debian/","section":"Tags","summary":"","title":"Debian","type":"tags"},{"content":" Introduction # Linux offers a multitude of powerful commands that make working on the command line efficient and effective. Whether you are managing files, monitoring system resources, or networking, knowing the right commands can save you a lot of time. This article introduces essential Linux commands and will be regularly updated with new commands and useful tips.\nFile and Directory Management # ls - List Files and Directories # Syntax: ls [OPTIONS] [DIRECTORY]\nDescription: Lists the contents of a directory.\nExamples:\nls – Lists all files and directories in the current directory. ls -l – Lists files in long format with detailed information like permissions, owner, size, and date. cd - Change Directory # Syntax: cd [DIRECTORY]\nDescription: Changes the current working directory.\nExamples:\ncd /home/user – Changes the working directory to /home/user. cd .. – Moves up one directory level. cp - Copy Files and Directories # Syntax: cp [OPTIONS] SOURCE DESTINATION\nDescription: Copies files or directories.\nExamples:\ncp file.txt /home/user/ – Copies file.txt to /home/user/. cp -r folder1 folder2 – Recursively copies folder1 to folder2. File and Text Manipulation # cat - Concatenate and Display Files # Syntax: cat [OPTIONS] [FILE]\nDescription: Displays the content of files.\nExamples:\ncat file.txt – Displays the content of file.txt. cat file1.txt file2.txt – Displays the contents of file1.txt and file2.txt one after the other. grep - Search Text # Syntax: grep [OPTIONS] PATTERN [FILE]\nDescription: Searches for a specific pattern in a file or input.\nExamples:\ngrep \u0026quot;error\u0026quot; logfile.txt – Searches for the word \u0026ldquo;error\u0026rdquo; in logfile.txt. ps aux | grep ssh – Searches for \u0026ldquo;ssh\u0026rdquo; in the list of running processes. System Information and Process Management # top - Display Running Processes # Syntax: top\nDescription: Displays a dynamic view of the running system, including CPU and memory usage.\nExamples:\ntop – Shows a real-time list of running processes. htop – A more user-friendly version of top (if installed). df - Display Disk Space Usage # Syntax: df [OPTIONS]\nDescription: Reports the amount of disk space used by file systems.\nExamples:\ndf -h – Displays disk space usage in a human-readable format. Networking # ping - Check Host Availability # Syntax: ping [OPTIONS] DESTINATION\nDescription: Sends ICMP echo requests to test the reachability of a host.\nExamples:\nping google.com – Checks if google.com is reachable. ping -c 4 192.168.1.1 – Sends 4 packets to the IP address 192.168.1.1. curl - Transfer Data from or to a Server # Syntax: curl [OPTIONS] URL\nDescription: Transfers data from or to a server, useful for testing APIs or downloading files.\nExamples:\ncurl https://example.com – Fetches the content of https://example.com. curl -o file.txt https://example.com/file.txt – Downloads file.txt from https://example.com. Tips and Tricks # Tips for ls # ls -lh – Shows file sizes in a human-readable format. ls -a – Lists all files, including hidden files (those starting with .). Tips for grep # grep -i \u0026quot;pattern\u0026quot; file.txt – Case-insensitive search for \u0026ldquo;pattern\u0026rdquo; in file.txt. grep -r \u0026quot;pattern\u0026quot; /path/to/dir – Recursively searches for \u0026ldquo;pattern\u0026rdquo; in a directory. Quick Reference (Cheatsheet) # Command Description ls List files and directories cd Change the current directory cp Copy files or directories mv Move or rename files or directories rm Remove files or directories cat Display file contents grep Search text top Display running processes df Display disk space usage ping Check host availability curl Transfer data from or to a server Updates # September 27, 2024: Added section on networking commands (ping, curl). October 15, 2024: Added new examples for grep and awk. ","date":"28 September 2024","externalUrl":null,"permalink":"/posts/linux-commands/","section":"Posts","summary":"","title":"Essential Linux Commands: A Growing Cheat Sheet for Everyday Use","type":"posts"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":" Privacy Policy # This privacy policy explains how I handle visitors\u0026rsquo; data and which tools are used on this website, such as Google Analytics. Protecting your personal data is very important to me, and I strive to ensure compliance with data protection regulations.\n1. General Information # The purpose of this privacy policy is to inform you about the type, scope, and purpose of the collection and use of personal data on this website. I, Emre Hayta, am the owner of this website and responsible for data protection.\n2. Data Collection # a. Server Log Files # When visiting this website, the web server automatically collects and stores information in server log files. These include:\nBrowser type and version Operating system used Referrer URL Hostname of the accessing computer Time of the server request IP address (anonymized) This data cannot be attributed to specific individuals and is only used for statistical purposes, to improve the website, and for security reasons.\nb. Google Analytics # I use Google Analytics to analyze user behavior on my website. Google Analytics uses cookies that enable an analysis of how you use the website. The information generated by the cookies about your use of this website is usually transmitted to a Google server in the USA and stored there. However, IP anonymization is activated on this website, meaning that your IP address will be truncated beforehand within the European Union or other parties to the Agreement on the European Economic Area.\nc. Contact Form and Email Communication # If you contact me via a contact form or by email, the information you provide (such as your email address, name, and message content) will be stored for the purpose of processing your request and in case of follow-up questions. This data will not be shared without your permission.\n3. Cookies # This website uses cookies to make the site more user-friendly, effective, and secure. Cookies are small text files that are stored on your device. Some of these cookies are \u0026ldquo;session cookies,\u0026rdquo; which are deleted after your visit, while others remain on your device until deleted. You can configure your browser to inform you about the use of cookies and allow them only in individual cases.\n4. Your Rights # You have the right to:\nAccess information about your personal data stored by me. Rectify incorrect data. Delete your personal data, provided there is no legal obligation to retain it. Restrict processing of your data. Object to data processing under certain circumstances. To exercise any of these rights, please contact me using the details provided in the imprint.\n5. Data Security # I take appropriate technical and organizational measures to ensure the security of your data and protect it against unauthorized access, alteration, or destruction.\n6. Changes to This Privacy Policy # I may update this privacy policy from time to time to reflect changes in legal requirements or my services. The current version will always be available on this website.\n7. Contact Information # If you have any questions about this privacy policy or the processing of your personal data, feel free to contact me via the contact form on this website.\nBy providing all this essential information, the privacy policy now complies with general legal standards, covering the types of data collected, the use of cookies, rights of the users, and other basic information relevant for ensuring transparency and compliance with data protection regulations.\n","date":"28 September 2024","externalUrl":null,"permalink":"/privacy/","section":"emr3.me","summary":"","title":"Privacy Policy","type":"page"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/terminal/","section":"Tags","summary":"","title":"Terminal","type":"tags"},{"content":"","date":"28 September 2024","externalUrl":null,"permalink":"/tags/unix/","section":"Tags","summary":"","title":"Unix","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS","type":"tags"},{"content":" Introduction # Amazon Web Services (AWS) is one of the leading cloud platforms, offering a wide range of services that allow businesses and developers to build, manage, and scale applications. For beginners, AWS can seem overwhelming at first, with over 200 different services to choose from. In this post, I’ll give you an overview of some commonly used AWS services and share useful tips for getting started.\nCommonly Used AWS Services # 1. Amazon EC2 (Elastic Compute Cloud) # What is it? EC2 provides scalable virtual servers in the cloud. You can choose from various instance types optimized for different workloads. When to use it? If you want to host your own applications or websites, EC2 gives you the flexibility to create and manage servers. 2. Amazon S3 (Simple Storage Service) # What is it? S3 is an object storage service ideal for storing large amounts of data like backups, images, or videos. When to use it? If you need secure, scalable storage. S3 is often used for backups and hosting static websites. 3. Amazon RDS (Relational Database Service) # What is it? RDS is a managed database service supporting multiple relational database engines like MySQL, PostgreSQL, and MariaDB. When to use it? If you need a relational database for your application but don’t want the hassle of managing it yourself. 4. Amazon Lambda # What is it? Lambda allows you to run code without managing servers. You only pay for the time your code is actually running. When to use it? When you want to build serverless applications or execute small, event-driven tasks. 5. Amazon CloudFront # What is it? CloudFront is a Content Delivery Network (CDN) that speeds up the delivery of content by caching copies at multiple locations globally. When to use it? If you have a global user base and want to optimize your website or app’s load times. 6. AWS IAM (Identity and Access Management) # What is it? IAM allows you to manage users, groups, and their permissions within AWS. When to use it? To ensure the security and access control of your AWS resources. Tips for Beginners # 1. Take Advantage of the AWS Free Tier # AWS offers a free tier for many services, allowing you to explore AWS without incurring extra costs. Be mindful that the free tier is limited in terms of time and usage.\n2. Learn the Basics of IAM # Security should be your top priority. Make sure you understand how IAM works to keep your AWS resources safe from unauthorized access.\n3. Utilize the Documentation and Tutorials # AWS provides comprehensive documentation and step-by-step tutorials. These resources are especially helpful for beginners looking to get familiar with specific services.\n4. Automate with AWS CLI and CloudFormation # For repeatable tasks, you can use the AWS Command Line Interface (CLI) or Infrastructure as Code (IaC) tools like AWS CloudFormation to automate your infrastructure.\n5. Set Up Budget Alarms # AWS offers flexible pricing models, but costs can quickly add up. Set budget alarms to keep track of your spending and avoid surprises.\nConclusion # Getting started with AWS can be challenging, but with the right resources and a clear focus, you can quickly gain momentum. The AWS services introduced here are some of the most commonly used by beginners. By familiarizing yourself with these and following the tips outlined, you\u0026rsquo;ll be well on your way to working successfully with AWS.\nHave any questions or want to learn more about specific services? Feel free to contact me!\n","date":"25 September 2024","externalUrl":null,"permalink":"/posts/aws-beginner-guide/","section":"Posts","summary":"","title":"AWS for Beginners: A Comprehensive Guide to Common Services and Essential Tips","type":"posts"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/cloudfront/","section":"Tags","summary":"","title":"Cloudfront","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/ec2/","section":"Tags","summary":"","title":"EC2","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/lambda/","section":"Tags","summary":"","title":"Lambda","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/rds/","section":"Tags","summary":"","title":"RDS","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/s3/","section":"Tags","summary":"","title":"S3","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/blogging/","section":"Tags","summary":"","title":"Blogging","type":"tags"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/github-pages/","section":"Tags","summary":"","title":"GitHub Pages","type":"tags"},{"content":" Introduction # In today\u0026rsquo;s fast-paced digital world, having your own blog is a great way to share knowledge, experiences, and ideas. In this guide, I will show you how to set up a static website using Hugo and host it for free on GitHub Pages. Hugo is a fast and flexible static site generator, and GitHub Pages makes it easy to host the site directly from your GitHub repository.\nBy the end of this guide, you will have a fully functional blog running on Hugo and deployed through GitHub Pages.\nWhat is Hugo? # Hugo is a powerful and fast Static Site Generator (SSG). It allows you to create websites that are completely static, which makes them fast, secure, and easy to host. With Hugo, you can write content in Markdown and easily customize your site\u0026rsquo;s design using templates and themes.\nWhy GitHub Pages? # GitHub Pages is a hosting service from GitHub that lets you publish websites directly from a GitHub repository. It\u0026rsquo;s particularly useful for projects, personal blogs, and documentation. The integration with GitHub makes it easy to manage changes and keep your site up to date.\nPrerequisites # Before we get started, make sure you have the following installed on your system:\nGit Hugo GitHub Account Step 1: Setting Up Your Hugo Site # First, create a new Hugo site on your local machine. Navigate to the directory where you want to store your blog files and run the following commands:\nhugo new site my-blog cd my-blog This will create the structure of your Hugo site in a directory called my-blog. Next, you\u0026rsquo;ll need to add a theme. In this case, let\u0026rsquo;s use the popular \u0026ldquo;Blowfish\u0026rdquo; theme. Run:\ngit init git submodule add https://github.com/nunocoracao/blowfish.git themes/blowfish echo \u0026#39;theme = \u0026#34;blowfish\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml Step 2: Create Your First Blog Post # Now that the site structure is ready, create your first blog post by running:\nhugo new posts/my-first-post.md This will generate a new Markdown file for your post inside the content/posts/ directory. You can edit this file with your favorite text editor and add the content for your first post.\nStep 3: Preview Your Site Locally # Before deploying your blog, you can preview it locally to see how it looks. In the root directory of your Hugo project, run:\nhugo server -D Open a browser and go to http://localhost:1313/ to see your site in action.\nStep 4: Preparing for Deployment to GitHub Pages # Now it’s time to deploy your blog to GitHub Pages.\n4.1 Create a GitHub Repository # Go to GitHub and create a new repository. You can name it .github.io to use it for GitHub Pages. If you want to host it on a subdomain, you can use another name like my-blog.\n4.2 Build the Static Site manually # Run the following command to build your site:\nhugo --minify --themesDir ../.. -d docs --baseURL https://username.github.io/ This will create a docs/ directory with all the static files of your site.\n4.3 Push to Github # cd my-blog git init git remote add origin https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repository\u0026gt;.git git add . git commit -m \u0026#34;Initial commit\u0026#34; git push -u origin main Step 5: Automating the Build with GitHub Actions # Instead of manually building and deploying your site every time you make a change, you can automate this process with GitHub Actions.\n5.1 Create the GitHub Action Workflow # In the root directory of your project (not the docs folder), create a directory called .github/workflows (if it does not exist already):\nmkdir -p .github/workflows Create a new file inside the workflows directory called gh-pages.yml and add the following content (adjust the URL):\ngh-pages.yml # Sample workflow for building and deploying a Hugo site to GitHub Pages name: Blowfish Docs Deploy on: # Runs on pushes targeting the default branch push: branches: [\u0026#34;main\u0026#34;] # # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read pages: write id-token: write # Allow one concurrent deployment concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: true # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.102.3 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_Linux-64bit.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Checkout uses: actions/checkout@v4 with: submodules: recursive - name: Setup Pages id: pages uses: actions/configure-pages@v5 - name: Build with Hugo env: HUGO_ENVIRONMENT: production HUGO_ENV: production run: | hugo --minify --themesDir ../.. -d docs --baseURL https://username.github.io/ echo \u0026#34;Build completed\u0026#34; ls -la docs - name: Upload artifact uses: actions/upload-pages-artifact@v3 with: path: ./docs # Deployment job deploy: environment: name: github-pages url: https://emrehayta.github.io/ runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v4 5.2 Commit the Workflow # Commit and push the workflow file to your repository:\ngit add .github/workflows/gh-pages.yml git commit -m \u0026#34;Add GitHub Actions workflow for Hugo\u0026#34; git push origin main Step 6: Configure GitHub Pages # In your GitHub repository, go to Settings Scroll down to the GitHub Pages section. Under Source, select the docs/ folder in our case (depending on your setup) Save the settings, and your site will be live! Conclusion # Congratulations! You\u0026rsquo;ve successfully set up a blog using Hugo and GitHub Pages. From here, you can continue customizing your theme, adding posts, and growing your site. Hugo and GitHub Pages offer a great, scalable way to maintain a static blog with minimal costs and maximum flexibility.\nHappy blogging!\n","date":"20 September 2024","externalUrl":null,"permalink":"/posts/hugo-article/","section":"Posts","summary":"","title":"Guide to Setting Up a Blog with Hugo and GitHub Pages","type":"posts"},{"content":"","date":"20 September 2024","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]